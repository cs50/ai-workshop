All right, this is CS 50, and this is already week 7 where wherein we introduce another programming language, this time known as structured query language or SQL or SQL for short. Now SQL, as we'll see, is a different sort of programming language that allows us to solve like a lot of the same kinds of problems that we've been dabbling with over the past several weeks, but arguably in a lot of contexts, it allows us to solve those problems more easily. Indeed, among the goals for. Today are to demonstrate that sometimes there's multiple tools that you can use to solve the same problem, whether it's C or Python or today's SQL but we'll also see that SQL allows us a different sort of approach to solving problems, whereas C very much so and Python to a large extent are very much procedural programming languages whereby you have to write these procedures, functions step by step that tell the computer what to do, including loops and conditionals and all of that. SQL is said to be a declarative programming language, which is a different sort of paradigm whereby when you want to solve some problem you essentially declare what problem you want to solve or you declare what question you have, and it's up to the programming language to figure out using loops and conditionals and all of those lower level building blocks, how to get you the answer. So ultimately today is all about teaching you yet another language, mostly so that you can learn again to teach yourself new languages and to appreciate that when. You exit a class like CS 15 or out there in the real world really isn't all that big a deal to pick up new programming languages, especially when in advance you've seen different programming paradigms like procedural, like object oriented, like today declarative as well. But today ultimately is also about data and so to get us started, we thought we'd collect some real world data by asking all of you a couple of questions. So if on your laptop or phone you would like to pull up this URL here. It will also exists in just a moment in QR code form. So if you'd like to go to that URL there or simply scan this here QR code with your phone, that's going to lead you to a Google form. For those unfamiliar, Google has lots of tools, among which are, uh, is a tool via which you can ask people questions via forms. Microsoft has something similar as well, and at that URL. What you'll soon see is a form that looks a little something like this among whose questions are which is your favorite language, at least among those we've studied thus far. So go ahead and anonymously answer the questions you see on this form you'll see which is your favorite language and also which is your favorite problem in problem sets thus far. And meanwhile, as you might know, if you've used Google Forms yourself to collect data, we can move from questions here to actual responses. And as people start to buzz in, we'll see that the data set here is starting to update in real time, and Google gives us these nice graphical user interfaces or GUIs via which we can analyze the data. And so far, Python is easily the winner with 70% plus of you preferring it, 11% of you, wishing we were still in scratch, and 18% of you in C, and you'll see the responses are coming in here, but for our purposes. Today, what's more interesting than the actual answers to these questions is how we can get at the raw data. So among the things you can do in Google Sheets is quite literally click View in sheets, which is in Google Forms, is click on View in sheets, and what this is going to allow me to do is access the underlying raw data. Now because Google has forms and spreadsheets, they sort of tied these two products together. But what's especially nice about Google spreadsheets is that I can also download the raw data as a file. I can download it as an Excel file, a text file, a PDF, but for Today we're going to download it in a very common format known as CSV for separated values. And indeed, if I go to the file menu download separated values, this is perhaps the most straightforward, easiest way to get raw data out of any kind of tabular data like this to load it into code that we are about to write. So if you haven't buzzed in already, that's fine, but at this point in time, now that I've clicked the button, I now have a CSV file in my Mac downloads folder which if I go ahead and open up here, I can see that indeed. I've got this long named file favorite form responses1.csv. I'm going to shorten that file name to just favorites.csv. And what I'm going to go ahead and do is open up VS code and in my File Explorer, I'm going to literally just drag and drop favorites.csv from my Mac. That's going to have the effect of uploading the file as it was at that moment in time so that we can now begin to write some code using this file. And VS Code is automatically gone ahead and opened it up for me. And what you're looking at here is what we're Start to call a flat file database. It's a very lightweight database in the sense that it stores a lot of data, and it's a flat file in the sense that it's literally just a text file. And by convention, the way the data is stored in this file is indeed by separating values with commas. There are other conventions as well, but CSV is probably the de facto standard. But TSV is a thing for tab separated values. PSV, which is pipe separated values where you might have a vertical bar. Essentially these file formats try to use a character that might not appear in the actual data so as to. Separate your rows and columns. So indeed if I switch back to VS code here and we take a look at the data, you'll see that from Google Sheets I've been given three columns time stamp, which was automatically generated for me, the language as well as the problem, and what I see here is that we had a few respondents buzz in a little early, very excited for today's data, but here's the rest of them from like 1:30 p.m. Eastern onward, and you'll see separating separated via commas are effectively three columns of data. So everything before the first column represents a time stamp. Every. Between the 1st and 2nd comma represents the choice of language that you all buzzed in with and then everything after the 2nd comma represents the problem. Now it's kind of jagged edges. It doesn't line up in nice rows and columns because some answers are longer, some answers are shorter, but the commas are sufficient to tell the code we write where one column ends and the next one begins. So how do we go about writing code like this if we'd now like to ask some questions about the data like what is the most popular language, what is the most popular problem, or. Conversely, the least of each of those, well, we could look at the original data in Google forms, and that's where we got the pie chart. But how is Google figuring out what the most popular answers are and what uh pie charts it wants to depict? Well, they probably wrote some code not unlike what we're about to do, although we'll start with just a command line environment as always. So within VS code, I'm going to go ahead and do this. I'm going to go ahead and open up a program called Favorites.ie and let's write a program whose purpose in life is to open the. CSV file. Read it top to bottom, left to right, and then crunch some numbers, figure out what the most popular answers are to those questions. So I'm going to go ahead and import a package that comes with Python, a library called the CSV library, and nicely enough, this is just code that someone else wrote years ago that figures out how to read data from a file separating it via comma so that you and I don't have to write all of that ourselves. Then I'm going to use this Pythonic convention with open quote unquote favorites.csv as file. If I want to be super explicit that I intend only to read this file, which is the default, I'm going to go ahead and explicitly say quote unquote R just like we did in C when using F open to open a file and read mode. And now I'm going to do this. I'm going to go ahead and say reader equals csV.reader file. So this is a Python convention whereby the CSV library comes with a function called Reader that takes as its sole argument here a file that has already been opened, and what that reader will do is Figure out where all of the commas are so that I can iterate over this reader in a loop and get back row after row after row without me having to write all of the code to figure out where those commas are. So what I'm going to do in this loop here in this block of code is for each row in that reader, let's go ahead and just print out maybe the second column which was the language column. So I'm going to go ahead and say print row 1 because what we'll see. Is that this reader, which again comes with Python, hands me a list, a list, a list for each of the rows wherein bracket 0 would represent the first column, 1 would represent the 2nd, 2 would represent the third because everything is 0 indexed in Python. All right, so let's see what the effect is here. Let me maximize my terminal window, run Pythonofas.ie, cross my finger that I got this right, and voila. There is every language that was selected by you all in the form from top to bottom by default chronologically. But there's a bit of a bug, I dare say. Let me scroll up and up and up in this output through all of these answers until I get to the very top where I ran the program myself, which is here, Pythonofavos.ie. There's a minor bug here. What's the bug in the output? yeah. Yeah, it accidentally includes the header, which is a bug in the sense that I really just wanted to see the languages, but the code is doing what I told it to, which is just print out every row. So there's a few ways we could ignore this. Let me go ahead and minimize my terminal window and let me go ahead and say, well, you know what, after we create this reader, let's just skip to the next. Uh, let's just skip to the next row and ignore it effectively and then begin iterating over everything thereafter. And so what happens now is if I re-maximize my window, rerun Python of favorites.ie, enter, and now scroll up again to the beginning of this incarnation of the program, you'll see that the very first thing I see after my program was run was indeed Python, Python, Python, Python, and so forth. No more quote unquote language. So how is that? Well, this is a feature we haven't quite seen before or talked about in much detail, but this reader is is stateful in some sense, and this was actually true of all of the file IO we didn't see whereby when you were using Fread or some other function to read data from the file, something was remembering where it was in the file so that you didn't get the same bytes again and again and again. It was more like a cassette tape, an old school cassette tape if you will, or a scrubber. Along the bar along the bottom of like any streaming video whereby when you just read some data it grabs the next chunk, the next chunk, the next chunk, the next drunk, and something inside of the computer's memory remembers where it is. So this says skip to the next row and thus when you do 4 row in reader, you get everything but the first row because the reader is stateful. It remembers where it is in memory. All right, well thus far this isn't all that useful because all I'm doing is just printing out the data, but let's take a step toward making this program a little more useful. Particular, let's just be a little more pedantic and specify that what I'm really doing here inside of this loop is figuring out what the current row's favorite is. So I'm going to create a variable called favorite and set that equal to row 1. And then, even though this doesn't change the functionality, I'm going to print that favorite just because semantically, stylistically, it's nice to know what row 1 is as by defining a variable that tells me or anyone else who reads this code in the future what it's actually doing. All right, but readers are only so useful, and in fact, if I were to open up this CSV file maybe in Microsoft Excel or Apple Numbers or Google Sheets again, you could imagine someone kind of moving the data by just dragging one of the columns to the left or the right such that now it's no longer time stamp language problem, maybe it's time stamp problem language or maybe time stamp is all. way over to the right, you can imagine therefore that the indices we're using 01 and 2 could be a little fragile because if someone changes the data on me now my code is just going to break because I am blindly assuming that the second column, aka 1 is going to be the language column, but that might not be the case. But there's an alternative to this. And you might recall having seen this before, I'm going to go into favorites.ie and tweak my code a little bit, not just to use a reader, but a dictionary reader. So I'm going to change this to dict reader instead of just reader. And then the upside of using a dictionary reader is that every time I go through this loop reading row by row by row, each row that I'm handed by this reader is not going to be a list anymore that's numerically indexed with zeros and ones and twos. Each row is going to be, as you might guess, a A dictionary, which is a collection of key value pairs, which means now we can use words as our indices instead of just numbers, which is to say if I switch from reader, which gives me lists to dict reader, which gives me dictionaries, I can change this line 10 now. And say I specifically want the language column wherever it is all the way to the left or the middle or the right so in general using a dictionary reader is probably just going to be more robust because it's resilient against changes in that actual numeric ordering. Alright, let me pause here to see first if there's any questions on this exercise whose purpose in life is just to demonstrate how we can download the CSV data then iterate over it line by line without actually analyzing it yet. No, OK, so let's ask maybe the most natural question, which is like how many people prefer Python? How many people prefer C or scratch in turn? In other words, how can we recreate in our own code what Google Forms is doing for us graphically with those pie charts? Well, I think what we could do is write some code logically that essentially relies on this mental model. What I have here is an opportunity to use a bunch of key value pairs because if I want to know how many instances of Python there are. and C and scratch. Well, those might as well be three keys, the values of which are hopefully going to be 3 numbers that represent the counts of the popularity of each of those languages. So in memory, I essentially want to construct something that looks like this and would if I were doing this on a chalkboard, but recall that this mental model maps perfectly to the notion of a Python dictionary, because a dictionary in Python is indeed key value pairs, and we've seen it already because that's how the dictionary reader works. But we could certainly use our own. Uh, dictionaries to solve the same problem ourselves. So the goal at hand is to count the number of people who said Python and C and scratch respectively. So how to do this? Well, I think what I could do is, I actually let me delete this line because we are using a dictionary reader, we no longer need to skip the first row. It is automatically consumed by the dictionary reader for us, so this now would be the better version of the dictionary reader. Let's go ahead and do this. Let me declare some variables first that will store for me the total number of people who said Python, scratch, and C respectively. So I could say scratch equals 0, C equals 0, Python equals 0, and I could just set 3 variables equal to 00, and 0. If you haven't seen it before, there are some pythonic tricks you can do here if you've got 3 variables that you want to initialize all at once because it's that simple. You could alternatively do scratch, C, Python equals 0.0.0. This too would have the intended effect and it looks a little better because it's all a simple one liner. But what do I want to do now? Well, down here, let's go ahead and do a simple conditional before we enhance this by using an actual dictionary. Let me go ahead and say if the current favorite in that reader equals equals scratch, well, Let's go ahead and increment the scratch variable by doing plus equals 1 as we saw last time. El if the favorite in the current row equals equals unquotec, well, let's go ahead and then increment the C variable by 1. E if the favorite equals equals Python, then let's go ahead and increment plus equals Python by 1 instead. I could technically get away with saying else here, but I'm consciously this time not trying to over optimize this because if someone changes the form maybe next semester and whatnot, and we're asking about 1/4 language, I wouldn't want my code to assume that anything that isn't scratch or C must be Python. When there could be some future 4th language. So this is a little more robust and in this case we'll just ignore anything that isn't scratch or C or Python. All right, at the end of this, let's go ahead and not just print out the favorite, but outside of the 4 loop, let's go ahead and print out, for instance, the scratch count is this, then let's go ahead and print out the C count is this, and then let's print out the Python count is this, but of course there's a subtle bug here. Yeah. Uh, so I didn't format these things. It's F strings, so I need the little F over here to the left of each of these strings. All right, so let me go ahead and maximize my terminal window, run Python of this version of favorite stoppi and hopefully what we'll see is not every row again and again and again, but 3 lines of output, giving me the total counts instead. All right, this seems to line up with the rough percentages that we saw coming in earlier on Google Forms, 109 of you like Python, followed by 58 of you and C and 24 of you preferring scratch instead. All right, but why does this perhaps rub you the wrong way? I already alluded to the fact that we're going to get rid of this, but why is this not the best design? Just using three variables like this, yeah. Yeah, exactly. If we were to add a bunch more languages, a 4th 1, a 5th 1, a 6th 1, a 10th 1, a 20th 1, like having that many variables, it's just certainly going to look unwieldy and it's just not gonna, it shouldn't rub you the right way. At that point we should really be graduating to some proper data structure whether it was an array in C or better still in Python, an actual dictionary. So let's do that instead. Let me go ahead and in a newer version of this file, let's get rid of these. variables and let's just have a generic variable called counts for instance and set it equal to an empty dictionary and just using two curly braces will give me an empty dictionary or if you want to be more pedantic, you can actually call the dict function which will return to you an empty dictionary. I'd argue though that most people would probably just use the double curly braces like this to indicate that here comes a dictionary for me now. Do I use this? Well, I don't need to update three separate variables. I think I could just do something like this. I could say once I've determined what the current row's favorite value is for language, I could say counts, bracket favorite, so use the current string as an index into the dictionary, so it's going to be quote unquote scratch or C or Python, and then just increment that by 1. And then down here we don't have these variables anymore, so I'm going to go ahead and instead say. Uh, how about this? We'll use a loop for each favorite in those counts, let's go ahead and print out, uh, how about the favorite value and the counts thereof without any F string for now. OK, so the only thing that's different is I'm using a dictionary here which is essentially the code version of this two column chart whose keys are going to be the favorite strings, scratch or C or Python, the values of which are going to be the actual counts, and I'm just doing some simple math by plus plusing or incrementing the count each time I see a certain language. Unfortunately this code is not quite going to work. Let me go ahead and run Pythonofavours.ie and Dang it, there's a key error. Let me minimize the terminal window so we can see both at once. Why is there a key error apparently on line 11, wherein I'm indexing into the counts array uh dictionary. What's going on? Yeah, yeah, it's a little subtle, but if this is like the very first time through the file, there is no key Python. There is no key C or scratch because no one has put them there and yet recall that plus equal means you're going to that location in the dictionary and just blindly incrementing it. But what is it? Well, it's effectively a garbage value, but it's not even that because there's no actual key there. So we need to do a little bit of logic here and we can solve this in a couple of ways. Well, I could say something very pedantically like this. I could just say, well, if this favorite is in the counts dictionary, this is the Pythonic way to ask that question is this key in this dictionary? If so, well then it's safe to go ahead and increment it just as I've done before. But if it's not, what I think I want to do is set counts favorites equal to. One instead because either I want to increment the current count by 1 or this is the first time logic like I've seen this favorite, so I want to set it equal to 1 instead. We could do this a different way logically just like we could in C solve problems differently. I could instead say something like this. I could get rid of all this code and just say if. Favorite not in counts, then I could say counts bracket favorite equals 0. So just always initialize it to 0 if it's not there. Now I can safely blindly update the count by 1 because now I know no matter what, once I get to line 13, that count is actually there. All right, so let's see with this version of the code, let's go ahead and clear my terminal window, uh, rerun Pythonofavos.ie, cross my fingers, and There we go, Python and Scratch and see. Interestingly, the order switched around this time based on the order in which I was inserting things into the dictionary, but we'll see how we can exercise a bit more control over that. But let me propose that that key error recall we discussed briefly last week that whenever you have these kinds of trace backs that refer to certain exceptions like. Exceptionally bad situations that can happen, you can also change your code to just try to do something and then try to catch the exception instead. So an alternative way to do what we initially did would be this instead of just blindly saying go into the counts dictionary, index into it at the favorite key, and increment it by one, what we could do is. Try to do that please, except if there is a key error, in which case, you know what, go ahead and just initialize that value to one instead. So in short there's like 4 different ways already to solve the same problem whichever way you prefer is quite reasonable. This is just another way and arguably another pythonic way to do things by trying to do something but anticipating that something in fact can go wrong. A while ago what? Correct. A while ago I removed next reader because that was only necessary for CSV.reader because that was just reading every row again and again, but when you use a CSV dictionary reader, that automatically consumes the first row because that's how the dictionary reader knows what the columns will be called and so you don't have to skip over it instead, a nice enhancement. Other questions. On what we've just done. Here All right, so let me propose that like writing this amount of code is kind of annoying just to ask a relatively simple question like what's the most popular language in this file, right? It's nice. It's sort of a step backwards from Google spreadsheets and Apple Numbers and Microsoft Excel where you could really just like highlight the column and it would just tell you the answer usually in the bottom right hand corner or you could use a function in one of those spreadsheet tools to ask the same question. So it's starting to feel like with almost 20 lines of code like maybe. There's a better way, and I dare say there is rather than use a flat file database, let's graduate already to what the world calls a relational database, and a relational database is simply data in which you define relations among your data, which isn't so much relevant now except that that time stamp is associated with that language is associated with that prefer favorite problem as well. But we'll see that data sets can be much more, much larger and more complicated and it might be valuable. We can actually express relationships across multiple pieces of data. In particular, let's introduce already a programming language called Structured Query Language or SQL for short, AKA SQL and SQL essentially only has 4 fundamental operations. So even though we're transitioning into a new language, by the end of today we're gonna transition out of the new language because there's only so much you can do. Now, as with any language, it's going to take time and practice or to sort of get a hold the hang of it, but take comfort in knowing that SQL really just supports four fundamental. operations and the acronym that the world uses is indeed CRD, which stands for Create, read, update, and delete. That is to say, when using a relational database, you can create data, read data, update the data, or delete data, and that's pretty comprehensive as to what's possible. Now what is an actual database? Well, generally speaking, a database is just a piece of software that's running on a computer somewhere inside of which is stored a whole lot of data and that database therefore provides you with access to that data. At any time, whether it's on your local Mac or PC somewhere in the cloud or to a whole cluster of web servers which we'll talk about in the weeks to come as we transition from command line tools to the web. Now technically in SQL, the commands you actually use to implement this idea of creating data, reading data, updating, and deleting data is almost the same, but for whatever reason, uh, the world chose the command select, which is equivalent to reading. Data so we'll soon see that there's a command in SQL that lets us select data which is equivalent to this idea of reading it, whereas the other three options refer of course to writing data that is changing data. Um, technically speaking, we'll be able to insert data into a database as we'll soon see, and we'll also be able to drop data altogether, not just delete individual rows but whole tables, so to speak, of rows instead. So what does this all mean? Well, let's go ahead and do, say, an example of using SQL to solve, to ask some relatively simple questions and begin to develop some muscle memory for using this new language. If I were to manually load a bunch of data into a proper database for SQL, I would actually use code like this. I would literally type Create table. I'd come up with the name of the table, aka sheet, and then I would specify every column that I want to put in that table. And here's where the vernacular changes. So whereas in the world of spreadsheets you have sheets, tabs that contain rows and columns, in the world of databases you have tables which are just rows and columns. It's different terminology, but it refers to conceptually the same thing. In CS 50 we're going to use a specific version of SQL known as SQL Light, which is like a lightweight version of SQL that's actually very commonly used in web applications, in mobile applications, but it doesn't have all of the bells and whistles or all of the scalability uh that your Oracle, SQL servers, Microsoft Access, Postgress, MySQL, those are just product. Names open source and commercial alike, which if you've ever heard of just represent bigger, faster versions of SQL databases, but we'll indeed use the lightweight version of it known as SQL Light, and the command we're going to start to run is quite literally SQL Light 3, which is version 3 of the same command which we've pre-installed into your code spaces for you. So let's go ahead and do this. Let me go ahead and run a command called SQL Light 3 which is going to let me create my very first SQL Light database, and I'm going to import into that database the CSV file that we downloaded from Google Forms. In other words, I'm going to load that same data set into a different program, an actual database, so that I can use a completely different programming language to ask questions about it instead of writing, as we just did some Python code. So let me go back into VS code here. Let me close my CSV file and my Python file. Let me reopen my terminal window and let me go ahead and run SQLite 3 space, and then the name I want to give to this database, which for instance will be favorites.db for database by convention. Enter. I'm going to be prompted to make sure I want to create this new file. Y for yes, enter, and now I'm inside of the data. Base running a command at a prompt that now says SQL light and then an angle bracket. I'm not going to be using any.SQL files for now, although you can actually write SQL code in separate text files. I'm actually going to use the database's interactive interpreter to just run all of the commands I want interactively by just typing them out. Semicolon enter. Type it out, semicolon, enter back and forth, but you can save all of these commands as you'll see in problems. 7 in files as well. Now how do I go about actually importing that CSV file into this lightweight database? Well, for this, I'm going to execute 3 commands, and any command in SQL Light that starts with a dot is specific to SQL Light. This lightweight version of SQL, anything that doesn't start with a dot is generalizable and will work on most any SQL database anywhere in the world, no matter the product you're using. So I'm going to go ahead and in my SQLite terminal I'm going to change my mode to CSV mode just to tell the database that I want to load some CSV data. I'm going to then literally import that data from a file called favorites.csv, which is the file we downloaded earlier and then uploaded to my code space. And now I have to specify the name of a table, so I'm going to call this table, aka sheet favorites just to keep everything consistent. And that's it. In the absence of an error message, everything probably worked fine. I'm going to do quit. That quits out of SQL light, but what you'll now see if I type LS is that not only do I have favorites. CSV, which I uploaded, favorites.ie, which we wrote a few minutes ago, but I also now have favorites.db, which is a database version of that same file. Now I can't actually see what's inside of it because if I go ahead and run a code of favorites.db. I'm going to see this file is not displayed in the text editor because it is either binary or uses an unsupported text encoding. This is to be expected because this database is stored essentially in the form of zeros and ones that the SQLite 3 program knows how to read, but it's not something that VS Code can just show me everything they're in. And generally storing data in binary is going to be more efficient than storing things purely textually because we're going to be able to use various data structures and algorithms that we've been talking about. For weeks more easily on that binary data. All right, so let's go ahead now and see what this import command did. I'm going to again maximize my terminal window. I'm going to go ahead and run SQLite 3 again, passing in favorites.db. Enter this time it already exists, so it just opened it without prompting me. And now I'm going to go ahead and type another SQL Light specific command called schema. The schema of a database is just the design of the database. It look like? What are the rows and columns and tables therein? So if I type dot schema, what I'm going to see is this SQL command Create table, if not exists, quote unquote favorites, which is the name of the table. Then in parentheses there are going to be apparently 3 columns, one of which is called Timestamp, the next of which is called Language, the third of which is called Problem, and each of those columns is going to be raw text. Now we'll soon see that it doesn't have to just be text, but when I use the dot import command. This is the default table that SQLite created for me. Soon we'll see that I can exercise more control, especially over the types of data that I'm putting in this database. But what's really nice about the import command is it could not be easier to convert a CSV file to a SQLite database so that now as we're about to see, we can use SQL on it instead of Python or any other language instead. OK, so how do we go about getting data from this database? Well, the first of our commands that we'll explore is that one called Select. So select data means to read data from the database, and in this sense it's going to be a declarative language because I'm just going to declare what data I want to select from the database, and I'm not going to worry about opening the file anymore or iterating. It with a 4 loop or a while loop or defining variables of the like, I'm just going to select syntactically what I want. So let me go back to SQLite here. Let me clear my terminal just to get rid of the past commands and let's do the first of these. Select star from favorites, and I regret to say the semicolon is back for the SQL code we're now writing. Enter and we will see a sort of Asky art version now so even better than the raw CSV file of all of the data that was inputted into this table. So select star from favorites is apparently selecting everything so the star in this context is a wild card of sorts. That represents all of the columns in the table. The table itself is called favorites, so I'm selecting all of the columns from the table called favorites, and here you have it with sort of simple Asy art first column, second column, 3rd column, chronologically listed because that's exactly how it was loaded into the database. All right, so if star is wildcard, what more can we do? Well, if you don't care about all of the columns, you can actually be a little more specific. So I could say instead select just the language column from the favorites table, semicolon, enter, and now I have just a single column of data that shows me one cell for every submission but not the time stamp or the favorite problem that that person put in. Or if I want to declare that I want a couple of columns so I can say select language and problem. but I don't care about the time stamp from favorites as such, and now you get two columns instead. So in short, rather than write the dozen or so lines of code that we earlier did with Python to open the file and then iterate over it with a reader, we just select what data we want from this here database. But even more powerfully, SQL comes with a whole bunch of functions built in quite like the spreadsheet software that you and I are already familiar with in the real world like Excel and Numbers and Google Sheets. SQLite comes with an average function, account function, distinct lower min, uh, max min, upper case, and so forth. There's a whole list of them. We'll play around with just a couple of these if we want to transform some of this data. Let me go back into VS code, clear my SQLite terminal, and suppose I just want to get the total number of rows in the favorites table, like how many people at the moment in time I downloaded the file, even if not everyone had quite buzzed in yet. Did I end up with in that file? Well, I could say select the count of all of the rows from the favorites table, semicolon, and now I'll get back a single cell, which gives me 272 submissions had come in the moment I downloaded that file. Suppose I want to see just to confirm that no one submitted bogus data which languages were actually among those typed in. Well I can select only the distinct. Languages that were typed in from the favorites table and now I get a unique list of languages that everyone buzzed in with irrespective of how many times. If I want to maybe get um how many distinct languages there are, if it's not as obvious as 3 here, I could select the count of distinct languages from the favorites table, and that would just tell me the answer. 3 is the total number of languages that are distinct. In that submission, so again it's easy to just eyeball this, but very quickly with single statements that are sort of English like left to right is enabling me to just select the answers I want to some of these problems. Well, what more can SQL do? Well, here is a bunch of other. A keywords that we can add to our SQL commands that allow us to control further what kind of data we're going to get back. We're going to be able to group data by similar values. We're going to check for not not just string equality but for fuzzy matching, checking if something is close to a string that we're looking for. We can limit the total number of rows coming back. We can order or sort the data by a certain column, and we can actually have predicates, so to speak, using a where which is similar in. It to an if condition, but a little more succinctly written instead. So for instance, let me go back to VS code here. Let me clear my terminal again and let me go ahead and select how many of you answered C is your favorite language without selecting all of the counts again. Let's just hit the nail on the head. So let's select the count of rows from the favorites table where the language. Selected equals quote unquote semicolon, and I get back a simple answer 58 of you buzzed in with the answer C. How many of you liked both C and very specifically the problem called hello world if you sort of that was the extent of your sort of um the passion for for code, let's go ahead and select the count of star from favorites where the language you typed in equals quote unquote C. Uh, and the problem you typed in equals quote unquote hello world semicolon, and it looks like 5 of you said your favorite language was C and your favorite program was hello world. Great. All right, so it's getting a little more interesting. What about the other version of Hello World where we called it hello it's me? Well, that one's interesting because I think it's gonna break my convention of using single quotes, which would be convention here in SQL. Whenever you're using a raw string, single quotes here would be the norm, but let's type this. Out so select count of star from favorites where language equals quote unquote C and the problem this time equals quote unquote hello it's me. So at a glance this is probably going to confuse SQLite 3 because does that middle apostrophe belong to the first one or the second one. is ambiguous and this is weird. In C we would solve this problem by putting a backslash in front of it and a so-called escape character. Different languages have different conventions. This one's a little weird, but in SQL light what you instead do is doubly single quote it. So putting two single quotes is the convention for escaping a single quote just because you got to remember or Google these kinds of things in the real. If you forget. Enter now I get back that. So it was not the case that any of you liked both C and that problem specifically. Well, what if we want to be a little more inclusive of either hello problem? Well, I could do this in this way, just like in my code spaces terminal. I can go up and down to go back through my history. Same thing in SQLite, so I can go back to commands to get up here. And let me go ahead and write something longer where the problem is hello world or the problem equals quote unquote hello, it's double apostrophe me single apostrophe semicolon and parenthesis so it's wrapped onto two lines here so it's a little messy, but I'm just logically saying where you buzzed in with C as your language and a problem of. A world or a problem of hello it's me E. It should be the same answer as before because none of you liked hello it's me, but I chose this syntax because I can actually make this a little cleaner. I can go and delete this whole parenthetical and just say we're language equals C and the problem is like quote unquote hello percent sign single semicolon. So this is a little weird too. It's just how SQL does this instead, but whereas previously I was using an equal sign to check for literal string equality, like literally those problem names like allows me to use wildcards, and it's not a wildcard quite like the previous use of the asterisk that we saw when you are using a Wildcard in a string in sequel you say percent sign to represent zero or more characters there so hello comma space percent is going to hopefully match this or the other problem that started with hello comma. So let me go ahead now and hit enter. The answer is still gonna be the same, but indeed it's demonstrative that that is how you could express yourself a little more generally if you wanted a pattern match like that. Questions now on any of these techniques, yeah. they have Uh, good question. Does it have to be capitalized when doing string equality, yes, but not with like. Like will tolerate case insensitivity, so upper case or lower case. 00, I see. Good question. So the capitalization, so stylistically in SQL, I would argue, and this is a stylistic convention in SQL, certainly for CS 50 and also for a lot of companies and communities in the world to uppercase your SQL keywords just to make them stand out from words that you and I. Chose is like the name of the table or the name of the columns they're in. This is just a convention. I would propose like always to be consistent, but for CS 50 and for Style 50's sake, I would propose that you indeed capitalize like this and frankly it just makes it easier to read to my eye because the SQL stuff jumps out and then the lower case stuff is specific to your data set. A good question. All right. How about another uh set of keywords that we saw on the screen earlier, namely grouping by. Well, suppose we have a data set like this, whereby, oops, we suppose we have a data set like this, whereby How does this go? Happy Halloween. Whereby here's just an excerpt from that table. So for as languages go, uh, say one of you liked C, 2 of you like 3 of you liked Python, and then now that we're introducing SQL, let's imagine that 2 of you now like SQL even better. So that's the extent of the data set. Wouldn't it be nice to be able to figure out how many of you like C or Python or SQL? Well, I could write some Python code, open the file, iterate over it using variables, using a dictionary, and those what 20 or. Lines of code we wrote earlier to answer this question. Wouldn't it be nice to just ask the SQL language to figure out how many of you like C, how many of you like Python, how many of you like SQL? We can do this by grouping these cells by common values. Let's group all of the Python rows together and all of the SQL rows together, and even though there's just one, all of the Crows as well. So how can we do this? Well, let me go back to VS code here and clear my terminal, and let's do this. Let's select every language, but it's respective count as well from the favorites table, but before you do any of that group everything by language. So this one takes a little more practice and getting used to, but this is simply saying select all of the uh it's saying look at the languages, essentially group all of the common languages together and then figure out what count that gives you for all. Of the grouped rows. If I hit enter here, we'll get an answer just like the Python code that took me 20 lines of code to write earlier. What's really happening though in the database is something a little bit like this. Notice, of course, that there's only one version of C. There's then 3 versions of Python and there's 2 examples of SQL, and the table I'm essentially building is to group all of those by identical values and then spit out the total counts here. Now on the screen it's just 13, and 2 in the data set. With some 200+ responses we have much larger answers, including Scratch instead of SQL right here. But this now sort of speaks to just how much more convenient it is to, if you want to ask a question like that, especially if the data set is more than a couple 100 rows, if your boss, for instance, in the real world has a CSV data set and wants you to analyze the data, well, you can literally download it, import it into SQLite, run one command, and boom, like you've got this analysis done if the extent of it is just to group the data and figure out. Uh, what kinds of, uh, accounts you have in the data set. All right, what else can we do? Well, we can play around with this a bit more. Let me go back here into VS code and propose that we could. Uh, order those results more than just the uh the default way. So let's go ahead and select the language and the count from the favorites table yet again. Let's group by language yet again, but this time let's order by the counts column in descending order. So it's a bit more of a mouthful and it takes some practice to memorize all of the syntax, but when I hit enter now, I get back the same answers, but Python. is at the very top of the list. Now Countar isn't necessarily all that self explanatory and indeed it's a little annoying that I have to write out Countar here at top right as well as in the beginning. So it turns out SQL also supports aliases. So if you want to change the temporary name of the column to be something else like N for number, well then I can actually define an alias with the keyword as order by N at the end. Of this statement and then hit enter and get back the same results too. And so if it's not sort of implicitly clear already, each of these SQL select commands is essentially giving me back a temporary table. This is not being saved anywhere like now it's gone from the computer's memory once I've actually gotten my answer, but it's essentially returning a subset of the tables that do exist in the computer's memory because that's what the import command did for. Me, it loaded the whole data set into memory, and now I have these temporary tables that are just containing the answers to questions I care about. And if you only care about the top one language, well, there's a limit keyword too. I can literally just say limit one at the end of that exact same statement enter, and now I've got the single answer to my question, a single row saying Python was the most popular with 190 people selecting that. All right, for now, I think that's enough on select. There's a few more keywords, but it really is just a matter of composing these building blocks, questions though on these capabilities. Fundamentally. No. All right, well, how about maybe inserting data instead? So here might be the canonical way to insert a row into a table in SQL. You literally say insert into then the name of the table, then in parentheses the one or more columns for which you have data, and then literally the word values, and then in another set of parentheses a comma separated list of the one or more values that you want to insert into those that. columns. So for instance, let me go back into VS code here and of course at the time we circulated this form a few minutes ago, we had not yet assigned problem set 7. But in problem set 7 is a problem called 50ville which let's propose might very well be someone's favorite in a week. So let's go ahead and insert that row now preemptively. Let's insert into the favorites table. Two columns language and problem. Why? Well, I don't really care to figure out what the time stamp is and the format thereof, so I'm just going to omit the time stamp altogether, but the values I'm going to insert for this new row are going to be are going to be quote unquote SQL 50 ville close close parenthesis semicolon. E. Nothing bad seems to have happened. Let me go ahead and select star from favorites just to see what my data set looks like now. And indeed at the bottom of the file or the bottom of the table, indeed there is that new row. But what's sort of noteworthy is that this isn't just blank. There's our old friend null, which is not a null pointer, it's the same word, literally NULL, and it refers explicitly to the absence of data. And this is actually a nice feature because if any of you have ever used like Google. Spreadsheets, Apple numbers, Microsoft Excel and thought about uh or looked at cells that are blank. Like what does it mean if a spreadsheet cell is blank? Does it mean like there's literally no data there? Does it mean that you just don't have the data there or it's missing in some form? Well, how do you address that? Well, maybe you put like Nlash A in English for like not available or something like that, but that's kind of hackish and if you use NA, that might mean that no one can actually type NA at the. Answer. And so what's nice about SQL and data and database languages more generally is that null signifies the conscious omission of data. It's not just a missing value, it's consciously not there. It's not just the empty string, quote unquote, for instance. So we might see different examples of that, but what's nice now is that I can distinguish null from other values and in fact if that is not a good idea to have any data in my data set. That is null for whatever reason like it just looks like bogus data. It would be nice to know who inserted that when, no problem. We can also delete data from a table in SQL, and I can delete from the name of the table where some condition is true. So for instance, if I want to delete that, I can do this in a couple of ways, but perhaps the simplest is to delete from. Favorites, where, uh, time stamp is null, semicolon. So is 2 is another SQL keyword here, and that will go ahead and delete only those rows where the time stamp is null. Enter. Let's do the same select command as before. Enter and voila, that row is now gone. Be very, very, very careful with delete statements. If I had foolishly done this, wanna guess what the results would be. It would delete everything and like you can Google around and see actual articles of like interns at companies who had way too much access to a company database executing something like delete from favorites because they forgot the predicate they hit enter too soon and boom, all of the data is now gone. So these are very destructive commands and just like in the real world if you don't have backups or versions of these same tables, the data can indeed be lost. So don't do that. Always have your wear and make sure your wear is correct. All right, well, let's go ahead maybe and um suppose let's claim that maybe 50ville is going to be a really popular problem among students, so much so that it becomes overnight everyone's favorite problem. Well, we can update the table as is. Here is the general syntax for updating rows in a table. You literally say update the name of the table. The word set and then a bunch of key value pairs. The column that you want to update setting it equal to the value that you want to update it to where some condition is true. So what does this mean concretely? Well, let's say that we want to change everyone's favorite to SQL and 50ville. I could do this update favorites, set language equal to SQL, problem equal to 50 ville. Close quote semicolon, and this is where again it can be dangerous, but in this case I'm gonna go ahead and hit enter without any predicate to filter this. Nothing bad seems to happen, but if I now do select star from favorite semicolon, all of you would seem to like 50ville, and there is no going back to the previous version of the table unless I quit out of this and I import the whole CSV again maybe after deleting the data entirely. All right, so how do I get rid of all of the data? Well, if you want to delete from favorites for real now, enter select star from. Favorites, we can confirm that that was a bad idea. There's literally no data in the database anymore, but we can certainly restore from our actual CSV. So in short, we've got select, we've got insert, we've got update, we've got delete, we've seen. 8 albeit automatically generated by SQL Light 3, maybe we'll see Drop and actually we can see Drop now. So recall that if I do. schema, I can see all of the tables in this database. If I do drop table favorites, semicolon, and now again do do schema, now there is nothing in this database at all. So that's an even worse command to run unless you know and intend what you're doing. Questions then on these crud operations creating, reading, updating, deleting yeah here first. Why do you not do quotation marks around null? So null is a special symbol, and if you put quotation marks around it, you would literally be looking for the value NULL that maybe was the name of a language or the name of a problem or something literally in the CSV. We are looking for the absence of that data altogether, yeah. Really good question, because it's so easy to destroy data like this, are people actively backing up their data? Short answer yes, absolutely. Like all of CS 50's web apps and the like are automatically backed up on some schedule. Even then we have to decide what that schedule is, and if it's daily, for instance, nightly, we could lose up to like 23 hours, 59 minutes of data in some case, maybe companies would therefore version their data more tightly, like every 5 minutes, every minute, although that's going to consume a lot more space, but there already is this theme of trade off certainly in computer. You can also implement forms of access control. So SQL Light is lightweight. It has no notion of usernames or passwords. If you have access to the data, you can touch everything, but in the real world with commercial and open source software like Oracle and SQL Server and Postgrass and MySQL, you actually have usernames and passwords and specific permissions, so you can give users interns the ability to select data but not update or delete or insert data or any combination thereof. So there are defenses. Other questions on these here crud. Commands OK, let's go ahead and play with some real world data. So many of you might be familiar with IMDB, the Internet Movie Database, which is a great repository of data for movies and also TV shows and actors and the like. And within IMDB's website you can actually download uh TSV files, tab separated values of files that contain a lot of the data from that website. So we went ahead and did this. We then converted that TSV. Data into a whole bunch of SQL tables so that we can begin to play with it in the context of TV shows. However, let's start first with a question about how you could go about modeling data for TV shows themselves. So for instance, in advance, I also created a few different spreadsheets that just allowed me to play with how I might model data, real world data at that. So the Office is very popular. Uh, TV show, the US version here is uh the US version here starred Steve Carell and others. So if I think about how IMDB or maybe just even little old me with a spreadsheet, might keep track of who starred in what TV show, well, I might just use a Google sheet like this, and in the first column have a title column where this is the title of the show like The Office, and then if it stars one person, I would put Steve Carell in the next column, but if. As a second star I might put Rainn Wilson or John or Jenna or BJ Novak here column by column by column, and I could just keep adding show after show after show after show one row per show and then however many stars that are in there. What might you not like about the design of this data though or what might start to look odd? Yeah, it's a little weird that we have star star star star star. Just this repetition has tended to be bad any time we're copying and pasting should rub you the wrong way. Other observations about it too? Yeah. Yeah, at the moment I've got 12345 stars, and there's certainly TV shows with fewer TV stars and more, and so, OK, I can add more columns. I can just keep saying star, star, star, but then it's going to be a very ragged data set, very sparse data set where there's gonna be a lot of blank cells for shows that have small casts, but then a lot of columns for shows that have large casts, so it just feels like mm, this should be rubbing you the wrong way. It just feels like it's gonna get messy, especially as the number of stars, let alone shows, gets. Larger. All right, well, another version of this, uh, data set that I put together is this instead. So I didn't like the fact that I was going to have an arbitrary number of columns based on the specific show in question. So here I scaled back and I just have a single column for title as before, but now a single column for star, and I decided that if a TV show has multiple stars, well I just put each of the stars' names and then to the left of them specify the show that they're in. Seems to be a little better and that I've solved some of the redundancy problem, but I've kind of just kind of like covered up the hole in a leaky hose and now another leak sprung up here, which is to say there's still a bad design. What's bad here? Yeah. Yeah, now I've got the office, the office, the office, the office, the office, and that too feels like I'm wasting space. If I manually type this in, odds are eventually going to screw up and one of these is going to be misspelled, which is going to break something somehow. So this too doesn't feel quite ideal. So the third and final version I whipped up to model this data, which is going to lead us to a similar design in an actual database looks a little more arcane but is the right way, at least academically, to do things, and we'll see technologically too this is going to be a big game. So here I now have a spreadsheet with 3 separate sheets. One is called shows, which is selected at the moment. Another is called People, which is not selected yet, and the third of which is called Stars. What am I doing here? Well, notice that in the show sheet I've still got the title column, but I've decided to give the office a unique ID, much like a Harvard student has a unique ID number, much like an employee in a company, probably. A unique employee ID. Similarly, have I given the office a unique identifier that happens to be the same as it is in IMDB. Meanwhile, for all of the people that exist in the world of TV shows, for instance, these 5 folks, I have their names as well as unique IDs for them, and those integers are unique to the people and no connection per se to the show IDs just yet. But the third and final sheet I've whipped up is going to be a sort of cross referencing sheet that allows me to associate shows with people, and at a glance this looks the most arcane of the three because it's just numbers, it's just integers, but if you recall from a moment ago that the office's unique ID was 386-676, well, that's how we associated that show with this person, which happens to be Steve Carell, and so forth. Now at a glance, not very useful. To me, the human, unless I do some fancy spreadsheet stuff like the lookups if familiar or the like, but this is a stepping stone to how proper databases do actually store data. What I have done here is normalize the data by eliminating all redundancies except for maximally some redundant integers. And why is that? Well, integers, at least we know from our days in C, are going to be a finite length. It's going to be 32 bits, maybe 64 bits, but it's always going to be the same number of bits, and that's nice because any time you have Fixed number of bits, it lends itself to storing things nicely in an array or doing binary search because everything is a predictable distance apart as opposed to strings like Steve Carell or John Krasinski or the names might vary in length. These IDs for the title of the show and these IDs for the persons are not going to vary in length because they're all just integers. But of course this spreadsheet now much less useful because if I want to figure out who is in the office, well first I have to figure out what show this is, then I have to figure out what. Uh, person this is and this is and this is, but that's where SQL is again going to swoop in and allow us to solve this problem. And indeed SQL is one of the most common ways that web applications today, mobile applications today store any amount of data at scale. They are most likely not using simple CSV files. They are using SQL L Lite or MySQL or Postgress or Oracle or other commercial and open source incarnations of SQL databases, and odds are IMDb might be using. The same as well. All right, so let's go ahead and do this. I have created in advance a file called shows.db that contains hundreds of thousands of rows from TV shows and TV stars and other data from IMDB itself, and in a moment we'll see a database that if drone as a picture looks a little something like this. There is going to be a people table, there's going to be a shows table. Going to be a star's table that somehow links the two. There's also going to be a writer's table and a ratings table and a genres table. So overnight this sort of escalated quickly from just favorites, which was a single table, to now a real world data set that has 6 tables. But here is the relational in relational databases as these arrows are meant to imply right now, there are relationships across these several tables. Case in point, here's people. Here and we'll see in a moment that a person in the IMDb world has an ID number, a name, and a year of birth. A show in the IMDB world has a unique ID, a title, the year it debuted, and a total number of episodes. But there's no mention of people in shows. There's no mention of shows and people. But for the arrows, there's going to be this third table here, stars that somehow links show IDs with person IDs, and this is. Where relational databases get really powerful because you can solve all of those redundancy concerns and actually enable yourself to select data much more quickly instead. But let's focus on something simple first. Let's focus just on the show's table, which pictorial might look a little something like this. So in just a moment I'm going to go ahead and reopen VS code, and I'm going to open up instead of favorites.db. I'm going to go ahead and open up. Uh, a file called shows.db which again I arrived with in advance. So if I open up with SQLite 3 shows.db and hit enter, I'm back at a SQL prompt. Let me go ahead and type schema shows just to show you what command created this here table, and it got a little more interesting already. Notice that the table is called shows and it's got 123. Four columns the ID for each show, a title for each show, the year it debuted for each show, and the number of episodes. There's also clearly some mention of types and some other keywords that we haven't yet talked about, but let's focus now first on just what the data is. The best way to wrap your mind around a new data set if someone hands you a SQL database or you've imported a CSV into a SQL database is just select some data. So select star from shows, semicolon. That's a lot of data flying across the screen. It's not very easy to see because some of the show names are apparently crazy long and so it's wrapping, but it's still going and going and going. I'm gonna hit control C to interrupt it. Control C, as with our terminals in general as your friend, let's run that same command but just limit it to the 1st 10 shows. So there are the 1st 10 shows in the IMDB database of TV shows. So we've got 10 rows in this data set going back to it looks like the 1970s is roughly. Where their data set starts. All right, so here's the data we have in here. Well, how much is there? Well, let's go ahead and check. So select count star from shows semicolon, and now we're talking. There's 250,000 87 shows in this database. And if I do the same for people, select count star from people semicolon looks like there are 704,315 TV. Stars associated with this year data sets. So here too, the data is much more interesting and much more representative of real world data. How about the ratings? IMDb, if unfamiliar, is also a place where you can go to check the ratings from users as to whether something is good, a good show, a bad show, or anything in between. So let's dotschema ratings and I'll see that, yeah, there's this table called ratings that as we saw briefly on the screen, there's a show ID. And then a rating and then the total number of votes that contributed there too and again some data types and other syntax that we'll get to before long. But let me go ahead and just do select star from ratings limit 10 just to get a sense of what the data is. That's now what the data looks like in that table. So to a human at a glance, not that useful because you don't know what those show IDs are, but in a moment we're going to see how we can reconstitute this data by linking these tables together. By way of those IDs and actually get answers to questions. So among other things, a SQL database or a relational database more generally supports 1 to 1 relationships whereby a row in one table can map to a one row in another table. So it's this is in contrast to one to many, for instance. So 1 to 1 means one row over here somehow relates to one row over here. Again, the relational in relational database. Uh, how might we go about. Uh, seeing this, well, first, here's a tour of the data types that SQL Light supports, whereas in C we had a somewhat similar list and in Python that list went away, at least with regard to explicit types. In SQL we're back to when creating our tables explicitly stating what the types of those columns are. So you have integers, you have numeric, which is more of a catch all for things like times and dates and other useful real world data. You have real numbers which are like floats with. points you have text, which we've seen already, and then you have blobs, which is a great name which stands for binary large objects. You can actually store raw zeros and ones like files in the database generally that's frowned upon to store files, but there's certain times where you do want to store binary data and not pure text. That's it for SQLite. There are only these 5 types in other commercial and open source SQL databases like Oracle and MySQL and Postress and the same names I keep rattling off, you have even more data types than these. So that's a the additional features you get by using other databases as well. There's a few keywords though that are worth noting in SQL. You can specifically say when creating a table that this column cannot be null. If you don't want time stamp, for instance, to ever allow for null values, you can literally specify when creating that table this column cannot be null. And if I try to insert data into that table with a null value as by not providing a time stamp, the insertion will fail. And so Here's where things are different from just writing Python code or certainly using a spreadsheet you can actually have built-in defenses so that you and no one else messes up your data by inserting bogus or blank data accidentally. You can further say that things must be unique, so every element, every cell in a column must be unique to ensure that you can't accidentally put two things with the same ID to Harvard IDs, to employee IDs that are duplicates. You can avoid that altogether. But more importantly. Relational databases support these two concepts primary keys and foreign keys, and this is where the magic really starts to happen. A primary key is the unique identifier for a table. It is the column of values that uniquely identify every row, so it's probably going to be the show ID, the person ID, the Harvard ID, the employee ID, any time you have a value, often numeric, often integral. All that uniquely identifies rows, you simply call that a primary key. When that same ID appears in another table for cross referencing purposes, you refer to it instead as a foreign key because that same key is over there in another table, thus foreign but they refer to one and the same things in the context of the table in which it's defined, it's primary if it appears in some other table, it is now considered foreign. All right, so how can we make use of this? Well, let me go ahead and propose that we execute a few SQL commands as follows. If I wanted to start asking questions about ratings, I could do something like this select star from ratings, where the rating is maybe a good show, so let's call it 6.0 or higher, but let's just limit this to the top 10 shows that meet that threshold. Enter. So here I now have a temporary table that gives me 3 columns from the ratings table show ID, which is a for the moment useless identifier because I don't know what show it corresponds to, but the rating value and the number of votes that contributed there too. Well, how might I actually get to the shows that are actually highly rated at 6.0 or higher? Well, I don't need to select star. If all I care about is these top 10, I can whittle this same command down to just selecting the ratings. And now, sorry, sorry. Not the ratings. I can whittle this uh this table down to just selecting the show IDs. So this is the answer to the question What are the top 10 TV shows whose ratings are 6.0 or higher? Well, from the table, these are the 1st 10 that come back, how do I now select. The shows that correspond to these values. Here's where things can be done a few different ways. I could select everything I know from the show's table where the idea of the show is in the following set. I'm going to do a parenthesis and then just for readability I'm going to hit enter. The and angle bracket just means. Continuing my thought, it's not executing the command yet. What is the query I now want to run? Well, it's going to be a nested query. I can now do the same thing as before select the show ID from the ratings table where the rating is really good, greater than or equal to 6.0, but let's then limit the total number of queries to just 10. So here, just like in sort of grade school math, we have parentheses. So the first thing that's going to be executed is the thing inside parentheses. So this is going to get me every show ID from the ratings table that has a really good rating of 6.0 or higher. That's going to return to me a column of values. I'm. Then gonna say select star from the show's table where the ID of the show is in that list of values but only show me 10 of those is what I'm asking here. So what I should now see is much more useful data, namely the 10 shows that are highly rated enter. And indeed I get back these 10 shows, all of whose ratings are indeed quite a bit higher. If I want to only care about the title, that too I can do. So let's do this again. Instead of selecting star, let's select title from shows where the ID of the show is in the following parenthetical select show ID from ratings where the rating is greater than or equal to 6.0. Close my parenthesis, limit to 10. E. And I see the exact same thing, but just the nail being hit on the head. Just give me the titles of those top several shows. Of course I might want to, might be able to do this differently. In other words, here's the top 10 titles. Well, what are the ratings? Like that's why you go to IMDB or Rotten Tomatoes or the like. You want to see the actual ratings, not the titles or the ratings. Well, it turns out we're gonna need another technique to do that, namely an ability to join two tables and in fact just as a teaser for this. If we want to start playing around with some real data, here might be, for instance, excerpts from two tables. Here's the show's table at left. Here's the ratings table at right, or a subset thereof. If I want to figure out what the rating is for a given show, wouldn't it be nice if I could somehow like line these two tables up together such that just like the tips of my finger, I line up this value with its corresponding value over here, a cross reference of sorts? Well, just for the sake of. And let me just kind of visually flip this around, though that does nothing technically underneath the hood. Let me just scooch them together now after highlighting the common values, demonstrate that, well, wouldn't it be nice to take the show's table and join it with the ratings table in such a way that those IDs all line up and we're going to have the ability to do just this. This is a lot already and this isn't the sort of cliffhanger I'd wanted to end on because who cares about joints, but it's going to be cool. But let's take our 10 minute Halloween candy break and come back in 10 for the next. All right, we're back. So recall where we left off was essentially here. We had these two tables, the show's table at left and the ratings table at right, and the motivation here was like how do we actually associate shows with their respective ratings because the ratings of course are not in the show's table. As an aside, they could be. And in fact, because this is meant to demonstrate a 1 to 1 relationship whereby every show has one rating, we could have just put the rating and the number of votes into the show's table, but we chose not to because uh you know I am. actually stores their ratings as a separate TSV file, and so what we tried to do for parity with that is only import into a ratings table the very TSV file that we had downloaded from them. But that too would be a solution there too. So at this point in the story we've got the shows table here. We've got the ratings table over here. We've noticed that there are commonalities. There are show IDs that appear in both tables and in fact, to use some of the new vernacular, this is the primary key, the ID column here. This is that same value. But in this context it's known as a foreign key because it's in some other table, but that's gonna be how we link these two things together. So how do we select for not just the Office but maybe every TV show, its respective rating? Well, let's go back to VS Code and at my SQL light prompt, let me go ahead and do this select star from the show's table, but let's go ahead and join the show's table with the ratings table. How do I want to join these two tables together? We'll do so on. The shows tables ID column being equal to the ratings tables show ID column and then go ahead and filter the results in the following way where the rating we care about should still be greater than or equal to 6.0, and let's only limit this to the top 10 results. So it's a bit more of a mouthful, but what I'm doing is selecting everything from the result of joining shows and ratings on. This column with this column and the rest of the predicate is as before. So join is going to do literally that. Join these two tables as I have prescribed. When I go ahead here and hit enter now that I have my semicolon, I get back a complete table containing everything from the show's table, everything from the ratings table with those unique identifiers lined up. Indeed, if you look at the primary key over here, the ID column 62614. Over here you have show ID which came from the ratings table 62614. So we've taken two tables and really joined them together, but we're only seeing a subset because I limited it to 10 such rows. Now of course most of this data doesn't seem very interesting if my whole goal is just to tell me what the ratings are for these shows. Well, let's go ahead and in code achieve this sort of result. Let's literally join these tables together. Let's get rid of the redundant. See altogether and then really let's whittle it down to just a title column and a rating column. So how do we do that? Well, in code I'm going to go ahead and select more specifically the title of every show and the rating of every show from the show's table, but I'm going to join it with the ratings table on shows. ID equaling ratings.show ID and as before, I'm going to limit it to where rating is greater than or equal to 6.0. And 10 such results. Enter and now I have a nice simple temporary table that in one column has the titles of these shows and in the right hand side has the ratings of the shows, even though those two data sets were completely separate in two separate tables. Indeed, if we think back to where this data came from, what we've been focusing on is the show's table, and we've joined it with the ratings table. Here's the primary key for shows. Here's the foreign key for ratings, and by convention notice that we've adopted a certain. Uh, a certain approach. Anything that's called ID here implies that it's a primary key. Anything that's something underscore ID implies that it's a foreign key, and the convention we adopted, which is actually quite common, is if the table is called shows plural, we call the foreign key show singularco ID. Different companies, different communities will have different practices, but we've been consistent across all of these tables with our underscore and lower case conventions. Yeah. Is all generate and relate to each other. Really good question. How do all these IDs generate and relate to each other properly? Well, in our case, I have no idea. The Internet Movie Database people came up with these unique identifiers somehow, and we simply incorporating them into our data set. In practice, what they probably did and what you will do, for instance, in future problem sets when generating data is you just assign an arbitrary integer starting at 1, then 2, then 3, then 4, then 5, and you just let it auto increment all the way up and you let the database ensure that you never have duplicate values, yeah. Just to clarify for the dot dot dot and er symbols, that's only to like make it look better, right? Like there's no like. Correct. The angled bracket that you keep seeing is just the continuation prompt, which means I have prematurely hit enter deliberately because I want to move everything onto the next line so it doesn't wrap ugly on to multiple lines. It is not SQL syntax. It's specific to SQL Light 3, and it's just a continuation of the thought. That's all. Good, good observation. Yeah. Um, when you limit it to 10, is it showing how does it earliest ones. Good question. When you limit something to 10, for instance, which ones do you get, you just get literally the 1st 10 rows from the table. And so it will typically be ordered if you don't use the order by uh keywords, uh, in the same order from which it came from those tables. And so you're just seeing arbitrarily the 1st 10 that match that predicate, which is rating greater than or equal to 6. We have not ordered it by rating, so I'm not getting like the 10 shows necessarily. I'm just getting the 1st 10 shows that are greater than 6, and the point for that is just I want it to fit on the screen rather than see hundreds of thousands of answers. OK, so you might recall now that there were certainly other tables besides these. So let's see in the broader scheme, not just shows and ratings, but let's focus on genres, if only because genres is interesting because it's no longer a 1 to 1 relationship because of course why would a show have multiple ratings that sort of has its own rating, but a show could certainly belong to multiple genres. You could imagine a show being a. and a drama or a musical and a comedy or any other number of combinations of one or more genres and so the way we've chosen to implement that here too is with a separate table called genres, which is not perfect. There's going to be some redundancies here that we have not yet eliminated, but it does indicate that we can go ahead and have multiple such values. Associated with each and every show. So how do we get there? Let's focus just on this. Let's go back in just a moment to VS code and let's take a look at the schema for now genres. In genres we have the following a table called genres, which has two columns a show ID, which is an integer that cannot be null, and a genre which is text, which is also not be null. And now for the first time, let's actually use some of the vernacular we've introduced. Here we have an example explicitly in SQL that specifies when creating this table that it shall. The show ID column shall be a foreign key that references the show's tables ID column, and admittedly I think the syntax for creating tables is a bit of a mouthful. Even I often have to read to look it up to remember the order of everything, but here we have the columns listed first and then these key constraints, foreign key referencing this primary key over here. And in fact, let's rewind to look at the show's table now to see from which from whence we came. So if I do dot schema. Of shows, which we've done before but waved our hand at it then, we'll indeed see that shows has a primary key called ID, which is an integer. How do I know that? Because the very last thing in the parenthesis says that the ID column in this table is a primary key. Then we see that the title is text can't be null. The year is numeric, which again I described as sort of a catch all for other real world numeric types that aren't purely integers or real numbers per se. Episodes is an integer. Both of those are. Apparently can be null because maybe IMDB just doesn't have that data for some older shows, but primary key is indeed specified here. And just for thoroughness, let me distinguish now genres from ratings. If I do do schema ratings again, which we waved our hand at earlier, very similar in spirit to genres in that there's an ID column that somehow references the show's table, and then some other column here genre. In this case we had ratings and votes which were reels and integers respectively. But notice this one additional constraint here. I deliberately specified that show ID in the ratings table must be unique. That is to say, you cannot have the same show ID more than once in the ratings table. Why? Because I indeed wanted a 1 to 1 relationship, and it would not be 1 to 1. If there were multiple show IDs that correspond to one ID in the show's table itself, but genres we're going to allow that it can be duplicates and so we don't have mention of unique there. All right, so where does this get us? Well, let me go back into my terminal here after clearing all of that, and let's go ahead and just see the data to wrap our mind around it a little more. Uh, real. So select star from genres, limit 10 just to see the the 1st 10. All right, so it looks like there's some comedies, avengers, comedies, family action, sci-fi, and so forth. Well, let's go ahead and look up just one show's information. In fact, I saw this number this ID before. How about let's just look up this show? What is this adventure show? 63881. So select star from shows where ID equals. 63881 semicolon. OK, so this is the show called Cat Weasel from 1970, which had 26 episodes in total, and that was indeed its unique identifier. So that's all fine and good if I want to see something about that specific show. But as before, how do I associate Cat Weasel in this case with all of its genres? Well, instead of it being a 1 to 1 relationship necessarily, maybe Cat Weasel is not just an adventure, maybe it's also a comedy and a family. Show and indeed if I go back to the results just now, you'll see that 68111 indeed lines up with adventure, comedy, and family and then the ID changes to be about some other show. So how do I select these three answers to the question What genre is cat weasel? Well, for this we need to talk about one to many relationships and how we can get those back. Well, let's go ahead and do this now in my terminal. Let me go ahead and say, Uh, the following select genre from the genres table where the show ID equals just that 63881, which I'm now starting to memorize adventure, comedy, and family. So that's the answer to the question, but this certainly isn't the best way to do this where you have to like look up the unique ID for the show you care about, then copy paste it or. and type it out into this query just to get the genres. It would be nice to just ask all of this in one breath. Well, we can do this even though it's a bit more verbose. I'm going to instead this time say select genre from genres where the show ID I care about equals, and now I'm just going to hit enter so as to move this nested query inside of parentheses, and I'm going to. Well, I don't know off the top of my head what the unique ID is for cat weasel, but I can ask the database select the ID from the show's table where the title of the show equals cat weasel, and this now obviates the need for me to memorize or copy paste that unique ID. I'll hit enter and close my parenthesis. Uh, I'm gonna go ahead then and say. Uh, semicolon, enter, and now I get back the exact same answers but without having to know or care about these numeric values and that's kind of the point here, even though the database itself, the actual IMDB website, needs to use these unique identifiers to store everything in the database we. Humans generally speaking should not know or care what these identifiers are. They're just meant to implement this notion of relationships, these cross references, and so here we see an example where you can ask the question you care about without worrying about any of the underlying numbers or even seeing them as a result. All right, well, what's really, how else might we go about doing this? Well, let me propose that we join these two tables and ask the question in a slightly different way. So here's an excerpt from the show's table. Here's an excerpt from the genre's table, and clearly we could do something like we did before for ratings where we could line these two up and kind of join them together just for the sake of discussion, let me flip these columns around though that has no technical significance, and now we can clearly see 63881. There and here. The difference though, because now this is a one to many relationship, is that it's not quite as simple as just joining the rows together. I need to kind of join it here and here and here, and the database can do this for you, albeit at some cost in redundancy. So what I'm going to observe is that these IDs are all the same primary key in this context, foreign key in this context. Well, I'm going to start to join them together here. But it's not possible to return a temporary table that's just outright missing data. You have to get the same number of rows and columns everywhere in a grid. So what the database is going to do if I do join these two tables together and they are participating in a one to many relationship with each other, it's going to duplicate the data that's necessary to sort of make every row look the same. Downside is it might indeed be taking up some additional space unless the database is smart and somehow using pointers or something. That underneath the hood to avoid the redundancy, but for my purposes this is actually quite nice because if I iterate over these rows as I could in Python, as we'll eventually see, it's just nice to have all the data you care about in each and every row even though it's clearly redundant, but the data is not being stored redundantly in the data, it's just temporarily being presented to me with this here redundancy. So what do I really want to have happen? Well, I really care about actually joining these two tables together and ultimately just getting back the title and the genre respectively, so let me go ahead and my VS code here and do select title. And genre from the show's table, but let's join it this time on the genres table on shows. ID equaling genres.ho ID. So that's quite the same as with ratings where uh the ID equals just for time's sake, 63881, which I know is cat weasel, but I could certainly use a nested query if I wanted to do this as before. Enter. And I get back cat weasel's 3 genres, and if I were to loop over this data in some kind of like Python code, I would have access to the title and genre with each iteration, which I claim is. Useful, but if I don't care about that and I just really want to select the genres, I can do this with joins too. Let me just select the genre from shows joining it on genres on shows. ID equaling genres.ho ID where the ID is cat weasel 63881. And now I get back just that answer. So in short, what have we just seen? One, you can join two tables together and whittle down the temporary table to just the data you care about. Or if you prefer, and if I scroll back up in my history here, you could take a fundamentally different approach but still get the same answer of simply using a nested query. I would say as you learn SQL for the first time, I think it's quite often easier to just do multiple. Nested queries because you sort of work your way from the inside out taking sort of baby steps to the problem. If the problem in question is give me all of the genres for a specific TV show. Well, first I need to know because I know how the data is laid out in the database. I need to know the unique ID of the show I care about. fine, that's pretty straightforward and hence this inner query. Once you have that you can parenthesize it and on the outside now you can select. The question to which you really want the answer, which is what is the genre that lines up with that show ID one or more times. So in short, nested queries probably easier and certainly when learning for the first time, but quite powerful are these joined queries where this achieves the exact same result, especially if I were to generalize away the 63881 and do a nested query here. Sometimes you want joins, sometimes nested queries suffice. What Oh my goodness, how does SQL do all of these searches? What's its time complexity? We'll talk about that to the end of today. In the most naive implementation, SQL is essentially just doing linear search from the top of the table all the way to the bottom. However, we as the programmers are going to have the ability to optimize those queries so that the database can actually do something closer to binary search and in general we'll be able to achieve much better performance as a result. A really good question. All right, let's go back to the big. Uh, flow chart of this data set we've looked now at shows and ratings. We've looked at shows and genres. Let's now focus on the juiciest part, like the part that associates shows with people, that is who stars in what, thinking back now to what I was mocking up in the Google sheet at the very start, whereby I wanted to somehow be able to associate the Office with Steve Carell and John Krasinski and Jenna Fischer and so forth, the right way. And the right way I claim is going to be like this. Here's my people table, which has a primary key of ID and then the name of each person and their birth year if known. Then we have the show's table, which we keep talking about, which again has a primary key, a title and year and episodes thereof. And then the stars table is somewhat new now because now when it comes to people starring in TV shows, we have a third and final type of relationship. A many to many relationship. Why? Because it's certainly the case that one person can be in multiple shows, and it's certainly the case that some shows have multiple people, hence many to many. So this is the 3rd and final relationship where just to recap, ratings was 1 to 1, genres was one to many, and now stars is going to be many to many. All right, let's dive in. So these queries will be a bit more verbose, but again they're gonna follow this principle of sort of taking baby steps to the answer we care about. Let me go back into VS code here and suppose I want to find out everything about the office that we know. So select star from shows where title equals quote unquote the office semicolon. Well, that's interesting. There's a whole bunch of offices. There was the UK versions. A few other variants, but the one we're probably talking about with these stars is the one that started in 2005 with 188 episodes. That's the US version, in fact. So let me be a little more precise. Let me select everything I know from the stars from the show's table where the title equals office and year equals 2005, so we don't confuse our answers with the other versions of The Office. Now how do I go about selecting all of the people who starred in that. version of the office. Well, I already have an answer to the question of what is the ID of that version of the office because it's right there in front of me. And in fact, I can narrow my query more precisely. Let's just select the ID from the show's table where the title is The Office and the year is 2005 386676. Now I could lazily just copy paste that or memorize it, but we're going to do this query more dynamically. I want to next though figure out who is in. That show. So if I have a show ID, I want to figure out who's in it, but how do I get to the people and the names of those people? I have to logically go through this cross referencing of the star's table. So here's where this query is going to be a bit meatier than the past ones and that we need to do a bit more work than before. All right, well, what's the work I need to do? Let me go ahead now. And do the following select all of the person IDs that are associated with this show ID. So how do I do that? Select person ID from the stars table where the show ID equals, and I could lazily copy paste this, but let's avoid that, where the show ID equals, let me now in parentheses do this, select ID from shows where title equals quote unquote the office and. And year equals 2005 and then close my parenthesis semicolon. So what am I doing? I'm taking a second baby step, if you will. The innermost query inside the parentheses is just again dynamically figuring out the unique ID of the office I care about. The outer query is now figuring out all of the person IDs associated with that show as per the stars table, and the stars table has only two columns show ID and person ID. That's how the linkage is done just with those integers. Enter. I now have a column of person IDs that are starring in that version of The Office, so how do I take this one final step if I really want to care about their names and not their random person IDs? Well, I could go ahead and select the name from the people. table where that person's ID is in the following set. So when I'm dealing with a single value, I just use equals for equality, but when I'm dealing with a whole result set, a whole column of answers, I use the preposition in in SQL instead. So where the person's ID is in the following data set. Well, let's do the same queries as before. Select all the person IDs from the stars table where the show ID I care about equals, because there's only one show I care about. I'm going to further parenthesize this select ID from shows where title equals quote unquote the Office and year equals 2005. Uh, I'll close my parenthesis. Enter. I'll close my parenthesis semicolon, and now from the outside in I've taken three baby steps. The innermost one just gets me the show ID. The second one in the middle gets me all of the related person IDs, and the last one is really the final flourish. Get me all of the names of these people based on those IDs. Enter and now we see all of the stars in this show beyond even the subset that we've been playing with visually on the screen. OK, that's a lot. Let me pause here and see if there's any questions. Yeah. This outermost query is what gives me the names, but that query needs to know the ID of the person who whose name you want. So the middle query actually gets all of those person IDs, but to get those person IDs, I need to know the show ID. So the innermost query, this one, gets me the show ID of the office itself. All right, so at the risk of overwhelming, here are other ways you can solve the same problem, but I do claim that the nested selects is probably conceptually and pragmatically the easiest way. But let's also solve this problem by doing a few joins just so you've seen it. Actually, before we do a joint, let's let's flip the question around first. How about all of the shows that Steve Carell has starred in besides The Office? So let me select everything I know from the people table where the name of the person. Equals quote unquote Steve Carell semicolon. All right, there seems to be only one Steve Carell in IMDB born in 1962. That's all nice and good. What I really care about is his ID, so I'm going to narrow this down to selecting just his ID. Now I could memorize or copy paste 136797, but don't need to do that. Let's just use this as part of a nested query. Let's now select all of the show IDs from the stars table that are somehow related. Need to Steve Carell's person ID. So where person ID equals, and I could copy paste this, but that's generally frowned upon. So let's not do that. Let's just set it equal to a nested query where I do the same thing as before. Select ID from people where name equals Steve Carell, then close my parenthesis semicolon. All right, he's been in a lot of TV shows, but this is not useful because I have no idea what all of these integers are. So the final flourish, select the title from the show's table where the ID of the shows I care about is somehow in this parenthetical list. Well, what's that parenthetical list? Well, select the show ID from stars where the person ID equals Steve Carell's. What is his ID? Well, I didn't memorize it, so I'm gonna select ID from people where the name of the person I care about is Steve Carell. I quote unquote close these this parenthesis, close this parenthesis, semicolon, enter, and now I see all of Steve Carell's shows. And even though we're doing this in a black and white command line environment, think about what the actual IMDB is doing with both of these queries. If you go to IMDb.com and search for Steve Carell, even though there's going to be a lot of colors and pretty pictures and whatnot, you'll probably get in some form a list of all of Steve Carell's shows. Or if you search for the Office, you'll get a list in some. Of all of the stars they're in, I could claim then that if IMDb.com is using SQL, which it very likely is, but not necessarily, they are executing queries just like we did. And when you type into the search box something like The Office or Steve Carell, they're essentially just copy pasting your user input into a prefabbed SQL query that they wrote in advance so as to get you the answers that you actually care about. So this is how a lot of today's websites and mobile apps are actually working. The programmer comes up with sort of the template for the queries you might ask, and then you supply the actual data you're searching for. All right, how about now, as promised, a couple of other ways to implement these many to many relationships, uh, based queries, but by using joints. If I know I need to involve the show's table, the people table, and the stars table, I can actually do this all in. One breath without any nested queries. Select for me the title from the show's table, but let's join that on the stars table on shows. ID equaling stars.sh show ID. Uh, But let's additionally join the show's table on the following. Let's join it on people on stars.person ID equaling people. ID. In other words, if you know conceptually that you've got these 3 tables, you want to somehow combine them without using nested selects, just figure out how to line them all up. So again, I'm selecting from the show's table, but I'm joining it with the stars table. By lining up the shows, tables, primary key with the stars tables foreign key, and I'm lining it up with the people table by lining up the stars tables foreign key with the people table's primary key. I'm just kind of logically connecting all of the things I know to be related and lastly, let's just say where the name I care about equals quote unquote Steve Carell semicolon. It's a little slower for now and this speaks to the question that was asked earlier, how is the database doing this? Well, slowly, apparently by default unless we optimize it, I got back essentially the same results although there is some duplication as a result, uh, which alludes to the. Um, filling in blank of blanks that I alluded to earlier. But let me show you one other technique too. But again, I would encourage you certainly for problems at 7 to focus on nested queries when you can because they're a little conceptually simpler. If I care about the titles of those shows, I could select title from the show's table and the stars table and the people table all at once in one breath. But I want to do so where the show's table's primary key equals the stars tables foreign key. Uh, and the people table's primary key equals the stars tables foreign key, and the name I care about is Steve Carell. In other words, this is just a third way to express the exact same idea by doing implicit. Joins by selecting data clearly from all three tables as per this comma separated list of table names but telling the database with your predicate, the where clause, how you want to line all of those tables up. If I hit enter here, cross my fingers, I should get back the same results as well, albeit with duplication, which I didn't see in the nested queries. OK, that too was a mouthful. Let me pause here for questions. Yeah. do that No the internal structure. Correct. In order to do this, you, as the programmer must know the internal structure of the database, which is quite often the case whether you created the database yourself or you work with a colleague who designed the schema for the database. That said, I think your question's hinting at sort of the challenge like I really need to know the underlying implementation details when really all I care about is the answers to my questions in code quite oftenly nowadays, um, there are object relations. Mappings whereby you can use ORMs for sure, whereby you can use libraries that they understand the underlying database schema you as the programmer do not need to because it figures out how to do all of the joints for you. So for CS 50 we're introducing everyone to the bottom up understanding of how these joints work, but that too can be easily automated because of those schemas. Yeah. That is Good question. Is indentation in SQL important? Technically no, but like with any of the languages we've talked about thus far, it is good for the humans and certainly good for the students in a context like this. Python of the languages we looked at is the most rigorous, whereby indentation very much matters and the consistency thereof SQL, I'm just trying to pretty print things to make it easy to grok visually. All right, so those last two queries were arguably kind of slow, whereas with my nested queries, I actually got lucky and just boom, I got the answer quite quickly. Those joins seemed to be a step backwards and that it was taking more time to get back the same data that I actually cared about, but that's something we can actually chip away at. It turns out that one of the other values of a relational database vis a vis something like a spreadsheet is that you can actually tell the database in advance how to optimize for certain queries. This is not the case for spreadsheets if you. Have a lot of data in Google spreadsheets or Microsoft Excel or Apple Numbers, tens of thousands of rows, hundreds of thousands of rows, millions of rows. Your computer's going to slow to a crawl, and at some point those software packages are just going to say sorry file is too big, and they're certainly not going to be terribly fast at searching the data. But with a SQL database and relational databases more generally, you are as much the architect of it as you are the user of it in this case. And so you can tell the database in advance if you want to. Optimize for certain queries like select statements. So for instance, let me go back to VS code here and just for the sake of discussion, let's time how long it takes to find all of the shows whose name is the Office. I'm gonna use a SQL like command called Timer and I'm gonna set it to on, and this is just now going to tell me for every command I run how long it took. I'm gonna now select everything from the show's table where the title of the show equals quote unquote the office. Semicolon Enter and that query took, let's say in real terms 0.042 seconds. That's crazy fast. Like it's less than a second. I mean it's truly a split second, so no big deal, but it's a fairly simple query. But I bet we could optimize even this. Now why would you want to optimize even queries that are already pretty fast? Well, if they're very commonly being executed, and I dare say someone going to IMDb.com and searching for The Office or any TV show, like that's the common case. People are looking for TV. Shows, movies, actors, and so forth, it'd be nice to use as little amount of time to answer those questions as possible. Why? One, it makes for happier customers and users because you're getting them the answer faster. Two, it saves you money because presumably if you've spent $1000 for a server and that server has a certain amount of RAM, a certain speed CPU or brain, it can only do so many searches per unit of time per second, per minute or the like. So wouldn't it be nice if all of those searches is faster. Using less time so you can handle not 1000 users at once but 2000 users or 5000 users all with the same hardware. So there's certainly upsides there. Well, how can I go about optimizing a query? Well, I can create my own index, another use of the create keyword in SQL, where I can tell the database to optimize for searches on a specific table and specific columns therein. I say create index, and then I come up with a name for the index, whatever I want. the name of the table that I want to index, and then in parentheses the columns that I want to optimize for. So what does this mean in real terms? Well, let's go back to BS code here and let me create an index called, for instance, title index, though the name doesn't matter, on the shows table using the title column. In other words, tell the database, please expedite searches on the shows tables title column. After all, that's what I just searched on. Enter. Now that took a moment, almost half a second, but that's a tab that's an index that only has to be created once. If I do a lot of updates and deletes, it might actually take a little bit of time over over the course of using the database to maintain that index, but for now that's a one-time operation creating the index. But watch what happens now. If I scroll up in my history and go to the exact same query as before, which previously took 0.042 seconds, which, yes, is fast but not nearly as fast as the new version. Which is 0.001 seconds instead. orders of magnitude faster so I can handle 42 times as many users on the same database, so to speak, than I could have previously just by building this index. So what actually is an index? Well, we come full circle to discussions in like week 5 of the class. So an index in a database is very often created using what's called a Bree. This is not a binary tree. B. The tree is its own distinct structure that's very similar in spirit in that it's fairly shallow because most of the nodes have children, but it doesn't necessarily have two children. It might have more children and in fact, the more children the nodes have, the sort of higher up you can pull all of the leaf nodes and the shorter you can make the height of the tree. So this is just a generic representation of a bee tree, but what this implies is that when I am now searching for titles like The Office, the database doesn't have to do the. Fault behavior which is start at the top and use linear search all the way to the bottom. If it has proactively built up an index in memory thanks to my command, it now has a tree-like structure storing those titles that allows it to find in some logarithmic time, whether it's logbase 2 or some other base, the same data much more quickly, and that's how we went from 0042 to 0.001 2nd instead in this case here. Questions then on these here indexes. No. All right, well, let's propose that we can combine some of today's ideas. It turns out that now we're getting to the point in the course where you're not just choosing between this language and another, you're generally using a suite of languages to solve problems. And indeed in the coming weeks of the class when we transition to web-based applications, you're going to use a bit of Python. You're going to use a bit of SQL, you're going to use a bit of JavaScript and two other languages called HTML and CS. You might be using like 5 different languages at a time just to build one application. Why? Because some of them are better for the job than others, and indeed that's the ecosystem in which real world software development is done. Well, to make this bridge, we have a version of the CS 50 library we're called for Python which has functions like Get string, even though it's not that useful because it's just like the input function, but get in and get float, but also in the CS 50 library. For Python, we have a module that specifically makes it easier to use SQL from Python code. After all, wouldn't it be nice if I could get the best of both worlds and implement like an interactive program in Python but that uses SQL to actually get back data? Or I can build a website that allows people to search for TV shows or TV stars and actually get that data from. A database but use Python to generate the web pages themselves. Well, we have some documentation for this library here, but I'm going to go ahead and use it in real time to show you how much more easily you can solve certain problems by using each tool for what it's good at. So let's go back to VS code here. Let me exit out of SQL light and get back to my normal terminal, and let me go ahead and let's say, minimize. My terminal here. Uh, actually, let's go ahead and open up favorites.ie, which is where we left off before, and recall that in the last version of favorites.ie we had simply used a dictionary to go about keeping track of how many of you said Python or C or scratch. And when I last ran this program with Pythonofavos.ie, the answer looked like this. Now notice that it's not sorted alphabetically, otherwise C would be first, and it's also. Sorted numerically, otherwise C would be second. So it would be nice in Python to maybe exercise some control over this, but I stopped short of doing that before because it gets very annoying quickly. And by this I mean the following. Let me go back into VS code here and into favorites.ie. And if I wanted to sort by the counts here, I could do this. I could change my Loop from iterating for favorite in counts to favorite in sorted counts. So this is actually not too bad thus far. I can actually sort dictionaries pretty readily. So now if I run this, and let me make my terminal a little bit taller so we can see both the results, if I run the program now, you'll see that it's sorted alphabetically by key. So apparently when you use the sorted function in Python and pass it a dictionary. Can still iterate over all of the key value pairs in that dictionary, but it's been sorted now by key. So that's nice if that's to be my goal, but maybe that's not really my goal. And here's how alternatively I could sort by value the 190, the 58, and the 24. I can still use the sorted function, but I need to tell Python to use a key, a sorting key of the counts dictionary's gets function, and then if I run it again. I now see it's sorted by value, but darn it, it's now sorted in the opposite order. I see scratch at 24, then 58, then 190. If I want to reverse it, well then I have to go up here and add another named parameter. Reverse equals true. I can run it another time and now I get the result. I. Long story short, this is just very annoying to have to use that amount of code to actually answer relatively simple questions and this is why we did transition for much of today to a declarative language like SQL that just let me select what I care about in that data. So if I again I go back into my database version with SQLite 3 of favorites. DB, I'll maximize my terminal window. What did we do before? Well, we can select uh from the database, uh, select. Uh, let's see, favorite com count star from favorites group by, uh, favorite semicolon, whoops oh it's fine. Oh, sorry. What do we do? We do select language com count star from favorites group by favorite. Oh damn it, what happened? Oh, we deleted it. See this is why you don't use the delete or drop commands. So I'm not going to demonstrate this again, but recall before break that when we last selected this information, we used the group by command to actually group by the language in question, and we got back all the counts, but then we were very easily able to reorder things by actually just using order by and then doing something in ascending order or for instance descending order. Well, now let's actually combine these worlds of Python and SQL together to write first a program that does just that. But to do this, we're going to need to restore that database. So let's go ahead and do this. Let's remove favorites.db, which is just a file in my account. Let's go ahead and run SQLite 3 of favorites.db. Create a new version thereof. Let's now go ahead and change my mode as we did earlier in class to CSV. Let's now do import of favorites.csv into a table called favorites, and now let's do do quit. And when I do LS, OK, now it's back, favorites.db in addition to today's other files. Now let me go ahead and run SQLite 3. Of favorites.db and just as a sanity check select star from favorites semicolon, there's all of the data back minus the addition and subtraction that we ourselves made earlier manually and let's go ahead and in SQL go ahead and do select language count star from favorites. And group by language, but let's order by count in descending order, and that's one of the last commands we ran with this file and there's the answer in a single line of code instead of some 17 lines of code plus or minus some white space here. Can we merge now these two ideas? Well, let's see how to do this. Let's go back into favorites. here and make a new and improved version of it that actually uses SQL and no dictionary, no for loop, no try accept or any of this. Instead, let's go ahead and from CS 50's own library, import a SQL function which will give me access to this functionality. Let's create a variable called DB by convention, but I could call it anything I want and set it equal to CS50 SQL function and pass to CS50 SQL function. The path to the database file I want to open. This is a little weird, but the syntax here is SQL Light without the 3, colon slash slash slash favorites. This syntax, otherwise known as URI, is going to allow us to use the SQL light uh protocol in order to open up favorites.db, which is the very file I was just experimenting with manually in my terminal. Here now is how I can execute a SQL query in Python using CS 50's library. Now, as an aside, even though this is indeed meant to be a training wheel, CS 50's library is just easier to use than a lot of the. World libraries that makes this possible. So because we spend so relatively little time on this, we're still using this training wheel for this. Give me a variable called rows because I want to get back all of the rows from this table that contain those languages and db. execute. The only function that's useful in the CS 50 library for SQL is this execute function which allows me to write literally a line of SQL like select language, count, star. Uh, from favorites, group by language, order by count, star. Uh, descending order just to make my life easier, I'm going to add that alias trick that we saw before. So as N to change the count to the variable n, and then here I can just do order by N instead. It's a little long, but notice that now I'm using SQL as a string that I'm passing as an argument to this DB execute function. So at the very end of this I've got to close my quote, close my parenthesis so as to use. One language in effect inside of another. Now assuming I do get back at temporary tables, rows with that line of code on line 5, let's do this for each row in rows, go ahead and do the following create a variable called language and set it equal to row, quote unquote language. Then create another variable called N for instance and set it equal to row, unquote N. and then let's just go ahead and print out language and N respectively. So what does CS 50's library do? It returns by design a list of rows. Each of those rows is a dictionary of key value pairs. So when I do 4 row and rows, this is just iterating over a list of values, and we've done that over the past couple of weeks. Inside of this loop, I'm just creating temporarily two variables, language and N, to show you that. Each row is indeed a dictionary, which means I can index into it using strings like quote unquote language and quote unquote n because those are the columns that I selected using this query up above. Strictly speaking, I don't even need these variables. I can just get rid of that and a little more succinctly just pass in row bracket language and then row bracket N instead. So let me go down to my terminal window here, exit out of SQL light, run Python of favorites.pi in this form, enter, and I get back, it would seem. The same exact answer, 190, 58 and 24 in this case. Questions now on this co mingling. Of languages All right, how about one final thing. Once we have the ability to like use Python, now we can in fact make things interactive. So for instance, let me close my terminal temporarily. Let me go ahead and now ask for some user input. So after opening the database, let's do this. Let's ask the human using Python's input function, or equivalently CS 50s get string function for their favorite TV. Show and store it in that same variable. Then let's do a SQL query that selects that data rows equals db. execute. Select and let's see how many people selected this favorite problem rather not TV show. How about favorite problem from our favorites data set. So select count star as N from the favorites database where the problem in question equals. Well, now I need to put the user's inputs. I don't know what that is yet because they haven't typed it in yet, so what I'm going to go ahead and do is a placeholder and say favorite, close quote, and make this whole thing an F string. Then I'm going to go down here and I don't need to iterate because ideally I'm just getting back a single answer. How many people chose this problem as their favorite? So I'm going to say. That the row I care about is simply the first row. So rows is a list, so rows 0 is the first and only row in that list, and then let's go ahead and print out row N. Let's see the results here and then see what happens. Let me put some single quotes here and single quotes here. Let me open my terminal. Let me do Python of favorites.ie. And I'll say hello world enter and as before at the start of class, 42 of you like that. However, this is not, not, not how you should ever write SQL code in Python. What could go wrong with this code? Nothing went wrong a moment ago, but what could go wrong? Yeah. The user input, how so? Injection True, I don't know what those are yet, but we're about to go there. What even more simplistically could go wrong by plugging in the user's input here? Yeah. Exactly. If I inputted the other problem we played with hello, it's me, where it was IT apostrophe S, that if interpolated right here is clearly going to confuse the single quotes such that who knows what's going to come back. Now in the best case, the code might just not work and I'll get some kind of error on the screen, which is not great for the user because the program's not going to be useful. There's no user friendly error message, but in the worst case, the user could do something incredibly malicious if you are simply blind. Blindly trusting user input and plugging their input into a SQL query that you yourself constructed. Why? What if the user types something crazy like the word delete or drop or update or any of those destructive commands that we saw earlier and somehow tricks your code into executing maybe the select, but then eventually an additional query like a delete. Maybe they type in a semicolon and then delete or a semicolon and then drop or something like that. This is. The biggest threat to taking user input and trusting it in the context of databases, and it's called, as one of your classmates knows already, what's known as a SQL injection attack. A SQL injection attack is the ability for an adversary or an unknowing user to somehow inject code into your database. A SQL injection attack then might look something like this in the real world. Here for instance is like the login screen. To GitHub.com, they do actually use SQL among other languages underneath the hood, I believe, not necessarily for this, but suppose they did. And when logging into GitHub.com, you're prompted for your username or email address, and then of course your password. Well, what if I know a little something about SQL and suppose for the sake of discussion GitHub is using SQL L Lite, which they're not using because it's not meant for massive, large massive data sets like this, but suppose they are, and just to be malicious, I type in my username maillin at harbor.edu. But then I use a single quote and then da da. Well, the single quote is there, me being an adversary in the story, because maybe I can confuse their code by closing their quotes sooner than they intended, and we haven't talked about this yet, but it turns out that in SQL is the comment character. So it's like hash in Python or slash slash C. This in SQL means ignore everything to the right. That alone can be used fairly maliciously as follows. Here, for instance, could be the code that GitHub is using underneath the hood, whereby they might have some Python code, and heck, maybe they're using the CS 50 library that executes this pre-made query. Select star from the user's table where the username equals this question mark and the password equals this question mark, passing in username and password, for instance. Uh, but if they are trusting the username and password I typed in and just plugging it right there, they could be vulnerable to indeed a SQL injection attack. For instance, this code we'll soon see is actually the right way to do it. But suppose they were doing it with F strings like I started to in my version of favorites.ie. Same thing, select start from users where username equals this username and password equals this password, and the little F here means here's a format string. What could go wrong? Well, let me actually. Paste in the mail at harvard.edu single text here. Notice that this single quote and this single quote are meant to surround the username and same thing for the password there. But watch what happens when I type in my data maillin at harvard.edu single quote. So this would seem to finish the thought prematurely, and then it says, and so that just means ignore everything else. And so the effect here is essentially to gray out all of that stuff because it's effectively been commented out. So what GitHub ends up doing accidentally in this case is selecting star from users where username is mailen@harvard.edu irrespective of what his password actually is. And if you assume that down here they've got some conditional logic like, well, if we get back some rows, that means that Malin is in fact a registered user, go ahead and log him in. We don't know what the code looks like, so it's you've just enabled anyone on the internet to log in as me or anyone else just by suffixing their input with a single quote. And and that's the least of our concerns. If we additionally went in there and maybe instead of we put a semicolon and then delete from users or drop users, we could cause massive havoc on their database. This happens all the time, even now in the current year, you can Google around and see examples of companies that have not used proper sanitization of user input, and it's not just the intern, it's like random people on the internet are accessing or destroying their data maliciously. So what is the solution to a problem like this? Well, one, do not use format strings in Python to simply plug in user input. But the more important lesson is never trust users' input. Either they're going to do something accidentally or they're going to do something maliciously, and you do not want that to happen. So the solution then is to use a library, almost always use a library. This is not a wheel you should reinvent yourself, and by library I mean something like this. If you instead. Use a library like CS 50s and you don't just use F strings, you'll see in a moment you use question marks. What will happen is this when the user goes and types in mail in at Harvard.edu single quote that's fine. Let them put weird scary characters like single quotes in their input. The library will take charge of escaping user input, so anything dangerous in their input will be changed from one single quote to two because we saw earlier today that that's how you escape a character, and that means that now what you have is an effect my username is apparently Malin@harvard.edu apostrophe, and that's my username. Well, that's obviously not a real email address. It's not a real username. This is just going to return false. No rows are actually going to come back. And the way to do this now in our favorites example analogously is in VS code here to actually go up into this execute line, don't use an F string, change the value of problem to be a placeholder instead, and then pass into this execute function. One or more arguments that will be substituted in for that question mark and this is not a CS 50 thing, this is a uh industry convention whereby you quite often use literally a question mark and that means that whatever this variable's value is will get plugged. Into that question mark for you, but the single quotes will be added. Any dangerous characters will be escaped for you, and at that point you can trust that the user can type in anything they want. Your code is not going to break. You can see hints of this actually in the real world if you've ever gone to a website. And they tell you like oh you can't like for passwords for instance, like all of us probably intuitively know that you should have pretty long uh hard to guess passwords with letters and numbers and punctuation symbols sometimes websites very stupidly prohibit you from using certain punctuation symbols, which should drive you nuts because there's no computational reason that you have to put the onus on the user to sanitize their own input, but quite likely those websites have kind of learned. Part of this lesson, and they know some characters can be dangerous in sequel like semicolons or single quotes or the like, and they just don't want you to ever type those in even though there are solutions to this problem. Use a library that someone else smarter than you with more history of writing code than you has used that's open source so that many people have seen it and banged on it over the years so that this problem is not something you're vulnerable to. Questions then on what these here SQL injection attacks. Are all about, yeah. also telling them what you're using and so maybe that. Good point. So if by also telling people what characters they shouldn't use, you're leaking information because a smart adversary might know, oh well, if they don't want me using that symbol, they're probably using this language or this technology. Yes, no good comes from telling the world more information than they need to know. So that's another good paranoia to have. How about one other issue before we come full circle to the SQL injection attacks. There's another challenge with relational databases and with SQL uh itself, namely race conditions. This isn't so much a problem when I'm writing a, a little program here on my own computer. Uh, but when you're running SQL code on a database in the real world in the cloud where you have many different servers talking to that database and many different users, uh, talking to those web servers, as is going to be the case at Meta and Google and Microsoft and any number of popular companies nowadays, and even some of CS 50 zone apps uses centralized SQL databases where if multiple people are trying to do the same thing on them at the same time, submit their homework, run Check 50, we too are vulnerable to what are called. Race conditions. So what is a race condition? Well, the way I learned this back in the day when taking a course on databases and operating systems more generally was to think of a scenario like this. Maybe in your dorm you and your roommates have a little dorm fridge and you're both in the habit of really liking to drink milk, as the story was told to us. And so maybe one of you comes home from class one day and you get to your room, look in the fridge, there's no milk in there, and so you decide to walk across the street to CVS or some other store to get milk. Meanwhile, your roommate comes home from there. Class and opens the fridge and it's like, oh, we're out of milk. Let me go to the store too. And for the sake of the story they go to a different store altogether so that you don't run into each other and the problem solves itself. So now both of you are on your way to a store to get milk. Time passes, you both come home, one of you puts a jug of milk in the fridge, the other one gets home and it's like, Oh damn it, like we already got milk. I can't fit this milk in the fridge, or now it's too much milk. We don't really like milk this much. It's gonna go bad. Like a very bad outcome here, having too much milk is the moral of the story. But what's the stupid story. What's the, what's the real takeaway? Why did we find ourselves in a situation where we ended up with too milk, too much milk? You know what the other We didn't know what the other person was doing, and to really geek out on this, we inspected the state of a variable that was in the process of being updated by someone else. And this is a thing in computing as far back as scratch. Recall with scratch, you could have multiple scripts running at the same time for a single sprite because Scratch in effect is multi-threaded. You can have a single sprite doing multiple. Things in parallel by having those multiple scripts. Similarly here, your room is sort of multi-threaded because you have two independent beings who can both go to the store to solve the same problem in parallel. The problem though is that if one is not aware that the other is doing that work already, you might make poor decisions. So in the real world, what should the first roommate have done after inspecting the state of the refrigerator and realizing, oh, we're out of milk. OK, call the other roommate or maybe more simply like put a note on the door or like maybe dramatically lock the refrigerator somehow and in fact that's a term of art in databases is to actually use a database lock so that if you are in the process of updating the value in the database, lock it so that no one else can inspect the value of that database and potentially make a poor decision. So when might this actually happen in the real world rather than the contrived milk example. So there are a lot of social media posts nowadays that are quite popular to this day, as of today, this is still the most popular Instagram post, for instance. And imagine when this was first posted hundreds, thousands, hundreds of thousands of. People might have all been clicking the heart icon essentially at the same time. Now Mehta, the company behind Instagram, presumably has lots and lots of different servers, but let's suppose for the sake of discussion they have a single database, which is not true, but the danger is still there even with multiple databases. All of these different web servers are talking to the same database, and suppose those those servers are using Python code and hey, the CS 50 library that might look a little something like this in order to decide how to update the total number of likes for an Instagram post. The first line of code running on meta servers. Might say this get these rows as follows. Execute a query like select the current number of likes from the posts table where the ID of the post is whatever it is 123456, whatever. Notice no SQL injection attacks possible here because I'm using the placeholder, not an F string. Then the next line of code running on MetaServer maybe just stores in a variable just to make the code more readable. The first rows likes column. So it's again it's the CS 50 library in the story. Rose is a list of dictionaries, so this is the first such element in the list, and this is the likes column in the column we just selected, the temporary. Lastly, what do we want to do? Well, we want a plus plus essentially that total, so we update the post table setting the number of likes equal to this question mark where the ID equals this question mark and we didn't see this already, but the CS 50 library supports indeed multiple arguments after the SQL string. I'm going to update the number of likes to be likes plus one, plugging in the same ID of that post. So in short, take on faith that it's quite common that in order to achieve one small goal like updating the number of likes stands to reason you might need to do 2 database queries or 3 lines of code. Now if these lines of code are executing on multiple web servers, you could certainly imagine that if people are hitting the like button pretty much at the same time, maybe one server is going to execute this first line of code and it's going to get its answer. Maybe there's 100 likes at this point in the story. And then just by chance. Another server, this line of code is also executed, but it too gets the same answer. There's currently 100 likes. Meanwhile, the first server in the story continues to do its execution of code such that it updates the number of likes from 100 to 101. But because the other server was essentially running the same code in parallel, it's going to make the same mathematical decision and update the number of posts, the number of likes from 100 to 1001. But at this point in the story, the number of likes should obviously be 10. 2, so we've lost data, and that's one of the dangers of a race condition is that you'll end up with an inaccurate result and for a company like Meta they don't want to go losing data like likes like this like that actually drives engagement and so forth and so like that's genuinely a technical, if not a business problem as well. So it's analogous to sort of the milk problem but actually at scale. So what's the solution? There's a bunch of different ways, but conceptually we just want to lock the database when this logic is being executed. such that when one server is updating the number of likes, no one else should be allowed to update the like count at the same time. Now that's a little crazy for someone as big as meta because you're really just serializing all of these likes and slowing things down. So there's more fine grain control nowadays, namely called transactions where you can essentially lock not the whole table and then certainly not the whole database, but just the row in question, for instance. And so you would use commands in SQL like begin transaction. And then execute the lines of code that you want, and then when you're ready to commit it, that is save it, you use a commit command, but if something goes wrong or you get interrupted, you can actually roll back the whole thing. And what this kind of code does in effect by using more verbose CS 50 and Python code like this is you can ensure that those three lines of code inside or technically the two database queries inside, will either both be executed or not at all. They will not be interrupted, and that's the fundamental solution to this problem analogous to putting a lock on the fridge or by leaving a note or calling your roommate, preventing them from making the same decision themselves. Questions then on these race conditions, the solutions again, even though this won't be germane for CS 50, simply using techniques like locks and what we called transactions. No. All right, then a final moment to end on. Uh, we would not be a computer science course if we didn't introduce you to a few pieces of CS Canon. Uh, here is a sort of meme that's circulated for years when it comes to like optical character recognition, OCR of like toll booths trying to detect your license plate automatically. This is someone trying to have a funny old time tricking the city into deleting their database altogether because if you're just scanning this off of someone's license plate in front of the car and just blindly plugging it in without sanitizing their input. Escaping their input with something like a good library, you might very well still drop the entire database. As an aside, something one did something similar to where I think they made their license plate null, NULL, which just confused the heck out of the system too because the programmers didn't understand why null was all over the place when lights were being run and whatnot. And lastly, a very famed character in the world of XKCD as computer science circles goes is this, so we'll end as we've done before on an awkward silence as you process this here canonical CS joke. Oh Now you two know who little Bobby Tables is. All right, that's it for week 7. We'll see you next time.